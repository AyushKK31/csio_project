{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this one should be used \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions_neww_updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the trained model and scaler\n",
    "with open(model_path, \"rb\") as model_file:\n",
    "    sgd_clf = pickle.load(model_file)\n",
    "\n",
    "with open(scaler_path, \"rb\") as scaler_file:\n",
    "    scaler = pickle.load(scaler_file)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the feature names the scaler was trained on\n",
    "expected_features = scaler.feature_names_in_\n",
    "\n",
    "# Ensure only the expected feature columns are used\n",
    "X = df[expected_features].copy()\n",
    "\n",
    "# Handle NaN values\n",
    "X.fillna(X.mean(), inplace=True)  # Replace NaNs with column mean\n",
    "\n",
    "# Scale the features\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = sgd_clf.predict(X_scaled)\n",
    "y_pred_proba = sgd_clf.predict_proba(X_scaled)[:, 1]  # Probability of class 1 (earthquake)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "df[\"Predicted Label\"] = y_pred\n",
    "df[\"Probability of Earthquake\"] = y_pred_proba\n",
    "\n",
    "# Save the results to a new CSV\n",
    "output_csv_path = \"predictions_neww_updated.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = \"sgd_classifier_model_updated.pkl\"\n",
    "scaler_path = \"scaler_updated.pkl\"\n",
    "data_path = \"sta_lta_stats_with_mer_alongmetadata_time_updated.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- Before Mean\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m X\u001b[38;5;241m.\u001b[39mfillna(X\u001b[38;5;241m.\u001b[39mmean(), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Replace NaNs with column mean\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Scale the features\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     20\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m sgd_clf\u001b[38;5;241m.\u001b[39mpredict(X_scaled)\n",
      "File \u001b[1;32mc:\\Users\\Ayush_trainee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Ayush_trainee\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1658\u001b[0m, in \u001b[0;36mRobustScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1645\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Center and scale the data.\u001b[39;00m\n\u001b[0;32m   1646\u001b[0m \n\u001b[0;32m   1647\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;124;03m    Transformed array.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1657\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1658\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1659\u001b[0m     X,\n\u001b[0;32m   1660\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1661\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[0;32m   1662\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m   1663\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1664\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1665\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1666\u001b[0m )\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_scaling:\n",
      "File \u001b[1;32mc:\\Users\\Ayush_trainee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Ayush_trainee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- Before Mean\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the trained model and scaler\n",
    "with open(model_path, \"rb\") as model_file:\n",
    "    sgd_clf = pickle.load(model_file)\n",
    "\n",
    "with open(scaler_path, \"rb\") as scaler_file:\n",
    "    scaler = pickle.load(scaler_file)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Ignore the first four columns (Date, Device ID, File, Threshold Time)\n",
    "X = df.iloc[:, 4:].copy()  # Select all columns from index 4 onwards (features)\n",
    "# Handle NaN values\n",
    "X.fillna(X.mean(), inplace=True)  # Replace NaNs with column mean\n",
    "\n",
    "# Scale the features\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = sgd_clf.predict(X_scaled)\n",
    "y_pred_proba = sgd_clf.predict_proba(X_scaled)[:, 1]  # Probability of class 1 (earthquake)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "df[\"Predicted Label\"] = y_pred\n",
    "df[\"Probability of Earthquake\"] = y_pred_proba\n",
    "\n",
    "# Save the results to a new CSV\n",
    "output_csv_path = \"predictions_with_time.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "with open(model_path, \"rb\") as model_file:\n",
    "    sgd_clf = pickle.load(model_file)\n",
    "\n",
    "with open(scaler_path, \"rb\") as scaler_file:\n",
    "    scaler = pickle.load(scaler_file)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Ignore the first three columns (Date, Device ID, File)\n",
    "X = df.iloc[:, 3:].copy()  # Select all columns from index 3 onwards\n",
    "\n",
    "# Handle NaN values\n",
    "X.fillna(X.mean(), inplace=True)  # Replace NaNs with column mean\n",
    "\n",
    "# Scale the features\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = sgd_clf.predict(X_scaled)\n",
    "y_pred_proba = sgd_clf.predict_proba(X_scaled)[:, 1]  # Probability of class 1 (earthquake)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "df[\"Predicted Label\"] = y_pred\n",
    "df[\"Probability of Earthquake\"] = y_pred_proba\n",
    "\n",
    "# Save the results to a new CSV\n",
    "output_csv_path = \"predictions.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
